void PrepareForUpdateForMerge(int *graph_new_dev, int *newg_list_size_dev,
                              int *newg_revlist_size_dev, int *graph_old_dev,
                              int *oldg_list_size_dev,
                              int *oldg_revlist_size_dev,
                              NNDElement *knn_graph_dev, int split_pos,
                              int graph_size,
                              NNDElement *debug_host_graph, int iteration_num, int *host_graph_old,
                              int *host_graph_new, int *host_size_old,
                              int *host_size_new, int *debug_parameters_shrink_graph, int *dbg_new_pre_sort_host, int *dbg_old_pre_sort_host, int *dbg_new_post_sort_host, int *dbg_old_post_sort_host,
                             int *new_reverse_all_dev, int *old_reverse_all_dev) {
  auto start = chrono::steady_clock::now();
  cudaMemset(newg_list_size_dev, 0, graph_size * sizeof(int));
  cudaMemset(oldg_list_size_dev, 0, graph_size * sizeof(int));
  cudaMemset(newg_revlist_size_dev, 0, graph_size * sizeof(int));
  cudaMemset(oldg_revlist_size_dev, 0, graph_size * sizeof(int));
  cudaMemset(new_reverse_all_dev, 0, graph_size * g_db_size/2 * sizeof(int));
  cudaMemset(old_reverse_all_dev, 0, graph_size * g_db_size/2 * sizeof(int));
  dim3 grid_size(graph_size);
  dim3 block_size(32);
  PrepareGraphForMerge<<<grid_size, block_size>>>(
      graph_new_dev, newg_list_size_dev, graph_old_dev, oldg_list_size_dev,
      knn_graph_dev, split_pos, graph_size, debug_host_graph);
  CUDA_ERROR_KERNEL_CHECK();
  cudaDeviceSynchronize();
  auto cuda_status = cudaGetLastError();
  if (cuda_status != cudaSuccess) {
    cerr << cudaGetErrorString(cuda_status) << endl;
    cerr << "Prepare kernel failed." << endl;
    exit(-1);
  }

  /*
  PrepareReverseGraphForMerge<<<grid_size, block_size>>>(
      graph_new_dev, newg_list_size_dev, newg_revlist_size_dev, graph_old_dev,
      oldg_list_size_dev, oldg_revlist_size_dev, split_pos);
  */

 //NEW: let's try to use my kernel for reverse graph preparation
  PrepareReverseGraphForMergeDeterministic<<<grid_size, block_size>>>(
      graph_new_dev, newg_list_size_dev, graph_old_dev, oldg_list_size_dev,
      new_reverse_all_dev, newg_revlist_size_dev,
      old_reverse_all_dev, oldg_revlist_size_dev);
   
  CUDA_ERROR_KERNEL_CHECK();
  // PrepareReverseGraph<<<grid_size, block_size>>>(
  //     graph_new_dev, newg_list_size_dev, newg_revlist_size_dev,
  //     graph_old_dev, oldg_list_size_dev, oldg_revlist_size_dev);
  cudaDeviceSynchronize();
  cuda_status = cudaGetLastError();
  if (cuda_status != cudaSuccess) {
    cerr << cudaGetErrorString(cuda_status) << endl;
    cerr << "PrepareReverseGraph kernel failed." << endl;
    exit(-1);
  }


  //Now for each reversed neighbors set we chose only the Merge_sample_num closest ones
  FinalizeReverseNeighborsKernel<<<graph_size, WARP_SIZE>>>(
      new_reverse_all_dev, newg_revlist_size_dev, knn_graph_dev, graph_new_dev);
  CUDA_ERROR_KERNEL_CHECK();
  cudaDeviceSynchronize();
  cuda_status = cudaGetLastError();
  if (cuda_status != cudaSuccess) {
    cerr << cudaGetErrorString(cuda_status) << endl;
    cerr << "FinalizeReverseNeighbors newRev kernel failed." << endl;
    exit(-1);
  }   

  FinalizeReverseNeighborsKernel<<<graph_size, WARP_SIZE>>>(
      old_reverse_all_dev, oldg_revlist_size_dev, knn_graph_dev, graph_old_dev);
  CUDA_ERROR_KERNEL_CHECK();
  cudaDeviceSynchronize();
  cuda_status = cudaGetLastError();
  if (cuda_status != cudaSuccess) {
    cerr << cudaGetErrorString(cuda_status) << endl;
    cerr << "FinalizeReverseNeighbors oldRev kernel failed." << endl;
    exit(-1);
  }
/*
  int val_newg_list_size, val_newg_revlist_size;
int val_oldg_list_size, val_oldg_revlist_size;

cudaMemcpy(&val_newg_list_size, newg_list_size_dev, sizeof(int), cudaMemcpyDeviceToHost);
cudaMemcpy(&val_newg_revlist_size, newg_revlist_size_dev, sizeof(int), cudaMemcpyDeviceToHost);
cudaMemcpy(&val_oldg_list_size, oldg_list_size_dev, sizeof(int), cudaMemcpyDeviceToHost);
cudaMemcpy(&val_oldg_revlist_size, oldg_revlist_size_dev, sizeof(int), cudaMemcpyDeviceToHost);

printf("Parameters passed to ShrinkGraphForMerge:\n");
printf("  newg_list_size_dev = %d\n", val_newg_list_size);
printf("  newg_revlist_size_dev = %d\n", val_newg_revlist_size);
printf("  oldg_list_size_dev = %d\n", val_oldg_list_size);
printf("  oldg_revlist_size_dev = %d\n", val_oldg_revlist_size);
*/
  constexpr int K   = MERGE_SAMPLE_NUM;           // 12
    constexpr int P   = MERGE_SAMPLE_NUM * 2;                      // 24 slots per node
    const size_t BYTES = graph_size * P * sizeof(int);

    // four snapshots
    int *dbg_new_pre_sort, *dbg_old_pre_sort;
    int *dbg_new_post_sort, *dbg_old_post_sort;
    cudaMalloc(&dbg_new_pre_sort , BYTES);
    cudaMalloc(&dbg_old_pre_sort , BYTES);
    cudaMalloc(&dbg_new_post_sort, BYTES);
    cudaMalloc(&dbg_old_post_sort, BYTES);

    // optional: zero-fill so unused cells are recognisable
    cudaMemset(dbg_new_pre_sort , 0, BYTES);
    cudaMemset(dbg_old_pre_sort , 0, BYTES);
    cudaMemset(dbg_new_post_sort, 0, BYTES);
    cudaMemset(dbg_old_post_sort, 0, BYTES);

    if(iteration_num == 0) {

     int total_nodes = graph_size;

    cudaMemcpy(host_graph_old, graph_old_dev,
    total_nodes * MERGE_SAMPLE_NUM * 2 * sizeof(int),
    cudaMemcpyDeviceToHost);

    cudaMemcpy(host_graph_new, graph_new_dev,
      total_nodes * MERGE_SAMPLE_NUM * 2 * sizeof(int),
      cudaMemcpyDeviceToHost);

    cudaMemcpy(host_size_old, oldg_list_size_dev,
      total_nodes * sizeof(int), cudaMemcpyDeviceToHost);

    cudaMemcpy(host_size_new, newg_list_size_dev,
      total_nodes * sizeof(int), cudaMemcpyDeviceToHost);
  }

   ShrinkGraphForMerge<<<grid_size, block_size>>>(
      graph_new_dev, newg_list_size_dev, newg_revlist_size_dev, graph_old_dev,
      oldg_list_size_dev, oldg_revlist_size_dev, dbg_new_pre_sort, dbg_old_pre_sort, dbg_new_post_sort, dbg_old_post_sort);



  CUDA_ERROR_KERNEL_CHECK();
  cudaDeviceSynchronize();
  cuda_status = cudaGetLastError();
  if (cuda_status != cudaSuccess) {
    cerr << cudaGetErrorString(cuda_status) << endl;
    cerr << "ShrinkGraph kernel failed." << endl;
    exit(-1);
  }

  if(iteration_num == 0) {
    /*
     cudaMemcpy(debug_host_graph, knn_graph_dev,
      graph_size * NEIGHB_NUM_PER_LIST * sizeof(NNDElement),
      cudaMemcpyDeviceToHost);
    cudaDeviceSynchronize();
    */
   int total_nodes = graph_size;

    cudaMemcpy(&debug_parameters_shrink_graph[0 * total_nodes], newg_list_size_dev,
           total_nodes * sizeof(int), cudaMemcpyDeviceToHost);

    cudaMemcpy(&debug_parameters_shrink_graph[1 * total_nodes], newg_revlist_size_dev,
           total_nodes * sizeof(int), cudaMemcpyDeviceToHost);

    cudaMemcpy(&debug_parameters_shrink_graph[2 * total_nodes], oldg_list_size_dev,
           total_nodes * sizeof(int), cudaMemcpyDeviceToHost);

    cudaMemcpy(&debug_parameters_shrink_graph[3 * total_nodes], oldg_revlist_size_dev,
           total_nodes * sizeof(int), cudaMemcpyDeviceToHost);

    cudaMemcpy(dbg_new_pre_sort_host, dbg_new_pre_sort,
           total_nodes * P * sizeof(int), cudaMemcpyDeviceToHost);
    cudaMemcpy(dbg_old_pre_sort_host, dbg_old_pre_sort,
           total_nodes * P * sizeof(int), cudaMemcpyDeviceToHost);
    cudaMemcpy(dbg_new_post_sort_host, dbg_new_post_sort,
           total_nodes * P * sizeof(int), cudaMemcpyDeviceToHost);
    cudaMemcpy(dbg_old_post_sort_host, dbg_old_post_sort,
           total_nodes * P * sizeof(int), cudaMemcpyDeviceToHost);

    cudaDeviceSynchronize();
  }

  auto end = chrono::steady_clock::now();
  if (VERBOSE) {
    cerr << "Prepare kernel costs: "
         << (float)chrono::duration_cast<std::chrono::microseconds>(end - start)
                    .count() /
                1e6
         << endl;
  }
}